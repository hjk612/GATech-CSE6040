{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d88d6bce3b214a79",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Association rule mining\n",
    "\n",
    "In this notebook, you'll implement the basic pairwise association rule mining algorithm.\n",
    "\n",
    "To keep the implementation simple, you will apply your implementation to a simplified dataset, namely, letters (\"items\") in words (\"receipts\" or \"baskets\"). Having finished that code, you will then apply that code to some grocery store market basket data. If you write the code well, it will not be difficult to reuse building blocks from the letter case in the basket data case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1e479ae0017ab834",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Problem definition\n",
    "\n",
    "Let's say you have a fragment of text in some language. You wish to know whether there are association rules among the letters that appear in a word. In this problem:\n",
    "\n",
    "- Words are \"receipts\"\n",
    "- Letters within a word are \"items\"\n",
    "\n",
    "You want to know whether there are _association rules_ of the form, $a \\implies b$, where $a$ and $b$ are letters. You will write code to do that by calculating for each rule its _confidence_, $\\mathrm{conf}(a \\implies b)$. \"Confidence\" will be another name for an estimate of the conditional probability of $b$ given $a$, or $\\mathrm{Pr}[b \\,|\\, a]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ea2ad5c7719c9ad4",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Sample text input\n",
    "\n",
    "Let's carry out this analysis on a \"dummy\" text fragment, which graphic designers refer to as the [_lorem ipsum_](https://en.wikipedia.org/wiki/Lorem_ipsum):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-556d0c072a973073",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 100 characters:\n",
      "  \n",
      "Sed ut perspiciatis, unde omnis iste natus error sit\n",
      "voluptatem accusantium doloremque laudantium,  ...\n"
     ]
    }
   ],
   "source": [
    "latin_text = \"\"\"\n",
    "Sed ut perspiciatis, unde omnis iste natus error sit\n",
    "voluptatem accusantium doloremque laudantium, totam\n",
    "rem aperiam eaque ipsa, quae ab illo inventore\n",
    "veritatis et quasi architecto beatae vitae dicta\n",
    "sunt, explicabo. Nemo enim ipsam voluptatem, quia\n",
    "voluptas sit, aspernatur aut odit aut fugit, sed\n",
    "quia consequuntur magni dolores eos, qui ratione\n",
    "voluptatem sequi nesciunt, neque porro quisquam est,\n",
    "qui dolorem ipsum, quia dolor sit amet consectetur\n",
    "adipisci[ng] velit, sed quia non numquam [do] eius\n",
    "modi tempora inci[di]dunt, ut labore et dolore\n",
    "magnam aliquam quaerat voluptatem. Ut enim ad minima\n",
    "veniam, quis nostrum exercitationem ullam corporis\n",
    "suscipit laboriosam, nisi ut aliquid ex ea commodi\n",
    "consequatur? Quis autem vel eum iure reprehenderit,\n",
    "qui in ea voluptate velit esse, quam nihil molestiae\n",
    "consequatur, vel illum, qui dolorem eum fugiat, quo\n",
    "voluptas nulla pariatur?\n",
    "\n",
    "At vero eos et accusamus et iusto odio dignissimos\n",
    "ducimus, qui blanditiis praesentium voluptatum\n",
    "deleniti atque corrupti, quos dolores et quas\n",
    "molestias excepturi sint, obcaecati cupiditate non\n",
    "provident, similique sunt in culpa, qui officia\n",
    "deserunt mollitia animi, id est laborum et dolorum\n",
    "fuga. Et harum quidem rerum facilis est et expedita\n",
    "distinctio. Nam libero tempore, cum soluta nobis est\n",
    "eligendi optio, cumque nihil impedit, quo minus id,\n",
    "quod maxime placeat, facere possimus, omnis voluptas\n",
    "assumenda est, omnis dolor repellendus. Temporibus\n",
    "autem quibusdam et aut officiis debitis aut rerum\n",
    "necessitatibus saepe eveniet, ut et voluptates\n",
    "repudiandae sint et molestiae non recusandae. Itaque\n",
    "earum rerum hic tenetur a sapiente delectus, ut aut\n",
    "reiciendis voluptatibus maiores alias consequatur\n",
    "aut perferendis doloribus asperiores repellat.\n",
    "\"\"\"\n",
    "\n",
    "print(\"First 100 characters:\\n  {} ...\".format(latin_text[:100]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a5563988504ab7d3",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "**Exercise 0** (ungraded). Look up and read the translation of _lorem ipsum_!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5c173f535da4a3b7",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "**Data cleaning.** Like most data in the real world, this dataset is noisy. It has both uppercase and lowercase letters, words have repeated letters, and there are all sorts of non-alphabetic characters. For our analysis, we should keep all the letters and spaces (so we can identify distinct words), but we should ignore case and ignore repetition within a word.\n",
    "\n",
    "For example, the eighth word of this text is \"error.\" As an _itemset_, it consists of the three unique letters, $\\{e, o, r\\}$. That is, treat the word as a set, meaning you only keep the unique letters.\n",
    "\n",
    "This itemset has three possible _itempairs_: $\\{e, o\\}$, $\\{e, r\\}$, and $\\{o, r\\}$.\n",
    "\n",
    "Start by writing some code to help \"clean up\" the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-aa9c675c7da751f1",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "**Exercise 1** (`normalize_string_test`: 2 points). Complete the following function, `normalize_string(s)`. The input `s` is a string (`str` object). The function should return a new string with (a) all characters converted to lowercase and (b) all non-alphabetic, non-whitespace characters removed.\n",
    "\n",
    "> _Clarification_. Scanning the sample text, `latin_text`, you may see things that look like special cases. For instance, `inci[di]dunt` and `[do]`. For these, simply remove the non-alphabetic characters and only separate the words if there is explicit whitespace.\n",
    ">\n",
    "> For instance, `inci[di]dunt` would become `incididunt` (as a single word) and `[do]` would become `do` as a standalone word because the original string has whitespace on either side. A period or comma without whitespace would, similarly, just be treated as a non-alphabetic character inside a word _unless_ there is explicit whitespace. So `e pluribus.unum basium` would become `e pluribusunum basium` even though your common-sense understanding might separate `pluribus` and `unum`.\n",
    ">\n",
    "> _Hint_. Regard as a whitespace character anything \"whitespace-like.\" That is, consider not just regular spaces, but also tabs, newlines, and perhaps others. To detect whitespaces easily, look for a \"high-level\" function that can help you do so rather than checking for literal space characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "normalize_string",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sed ut perspiciatis, unde omnis iste natus error sit\n",
      "voluptatem accusantium doloremque laudantium,  ...\n",
      "=> \n",
      "sed ut perspiciatis unde omnis iste natus error sit\n",
      "voluptatem accusantium doloremque laudantium  ...\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "def normalize_string(s):\n",
    "    assert type (s) is str\n",
    "    #\n",
    "    # YOUR CODE HERE\n",
    "    #\n",
    "    return s.translate(str.maketrans('','',string.punctuation)).lower()\n",
    "\n",
    "    \n",
    "# Demo:\n",
    "print(latin_text[:100], \"...\\n=>\", normalize_string(latin_text[:100]), \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1694"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(normalize_string(latin_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "normalize_string_test",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(Passed!)\n"
     ]
    }
   ],
   "source": [
    "# `normalize_string_test`: Test cell\n",
    "norm_latin_text = normalize_string(latin_text)\n",
    "\n",
    "assert type(norm_latin_text) is str\n",
    "assert len(norm_latin_text) == 1694\n",
    "assert all([c.isalpha() or c.isspace() for c in norm_latin_text])\n",
    "assert norm_latin_text == norm_latin_text.lower()\n",
    "\n",
    "print(\"\\n(Passed!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1aeaeca8482415e3",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "**Exercise 2** (`get_normalized_words_test`: 1 point). Implement the following function, `get_normalized_words(s)`. It takes as input a string `s` (i.e., a `str` object). It should return a list of the words in `s`, after normalization per the definition of `normalize_string()`. (That is, the input `s` may not be normalized yet.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "get_normalized_words",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First five words:\n",
      "['sed', 'ut', 'perspiciatis', 'unde', 'omnis']\n"
     ]
    }
   ],
   "source": [
    "def get_normalized_words (s):\n",
    "    assert type (s) is str\n",
    "    #\n",
    "    # YOUR CODE HERE\n",
    "    #\n",
    "    return normalize_string(s).split()\n",
    "\n",
    "# Demo:\n",
    "print (\"First five words:\\n{}\".format (get_normalized_words (latin_text)[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "get_normalized_words_test",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(Passed.)\n"
     ]
    }
   ],
   "source": [
    "# `get_normalized_words_test`: Test cell\n",
    "norm_latin_words = get_normalized_words(norm_latin_text)\n",
    "\n",
    "assert len(norm_latin_words) == 250\n",
    "for i, w in [(20, 'illo'), (73, 'eius'), (144, 'deleniti'), (248, 'asperiores')]:\n",
    "    assert norm_latin_words[i] == w\n",
    "\n",
    "print (\"\\n(Passed.)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-518787cb33ce90f5",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "**Exercise 3** (`make_itemsets_test`: 2 points). Implement a function, `make_itemsets(words)`. The input, `words`, is a list of strings. Your function should convert the characters of each string into an itemset and then return the list of all itemsets. These output itemsets should appear in the same order as their corresponding words in the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "make_itemsets",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def make_itemsets(words):\n",
    "    #\n",
    "    # YOUR CODE HE\n",
    "    #\n",
    "    return list(map(set, words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "make_itemsets_test",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7] error --> {'o', 'r', 'e'}\n",
      "[132] eos --> {'o', 's', 'e'}\n",
      "[68] sed --> {'s', 'd', 'e'}\n",
      "[228] non --> {'o', 'n'}\n",
      "[159] sunt --> {'t', 's', 'n', 'u'}\n",
      "\n",
      "(Passed!)\n"
     ]
    }
   ],
   "source": [
    "# `make_itemsets_test`: Test cell\n",
    "norm_latin_itemsets = make_itemsets(norm_latin_words)\n",
    "\n",
    "# Lists should have the same size\n",
    "assert len(norm_latin_itemsets) == len(norm_latin_words)\n",
    "\n",
    "# Test a random sample\n",
    "from random import sample\n",
    "for i in sample(range(len(norm_latin_words)), 5):\n",
    "    print('[{}]'.format(i), norm_latin_words[i], \"-->\", norm_latin_itemsets[i])\n",
    "    assert set(norm_latin_words[i]) == norm_latin_itemsets[i]\n",
    "print(\"\\n(Passed!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4c74b6b365ee1130",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Implementing the basic algorithm\n",
    "\n",
    "Recall the pseudocode for the algorithm that Rachel and Rich derived together:\n",
    "\n",
    "![FindAssocRules (pseudocode)](https://ndownloader.figshare.com/files/7635700?private_link=3c473609741895a5cc2c)\n",
    "\n",
    "In the following series of exercises, let's implement this method. We'll build it \"bottom-up,\" first defining small pieces and working our way toward the complete algorithm. This method allows us to test each piece before combining them.\n",
    "\n",
    "Observe that the bulk of the work in this procedure is just updating these tables, $T$ and $C$. So your biggest implementation decision is how to store those. A good choice is to use a dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4c052d77d909f1d8",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Aside: Default dictionaries\n",
    "\n",
    "Recall that the overall algorithm requires maintaining a table of item-pair (tuples) counts. It would be convenient to use a dictionary to store this table, where keys refer to item-pairs and the values are the counts.\n",
    "\n",
    "However, with Python's built-in dictionaries, you always to have to check whether a key exists before updating it. For example, consider this code fragment:\n",
    "\n",
    "```python\n",
    "D = {'existing-key': 5} # Dictionary with one key-value pair\n",
    "\n",
    "D['existing-key'] += 1 # == 6\n",
    "D['new-key'] += 1  # Error: 'new-key' does not exist!\n",
    "```\n",
    "\n",
    "The second attempt causes an error because `'new-key'` is not yet a member of the dictionary. So, a more correct approach would be to do the following:\n",
    "\n",
    "```python\n",
    "D = {'existing-key': 5} # Dictionary with one key-value pair\n",
    "\n",
    "if 'existing-key' not in D:\n",
    "    D['existing-key'] = 0\n",
    "D['existing-key'] += 1\n",
    "   \n",
    "if 'new-key' not in D:\n",
    "    D['new-key'] = 0\n",
    "D['new-key'] += 1\n",
    "```\n",
    "\n",
    "This pattern is so common that there is a special form of dictionary, called a _default dictionary_, which is available from the `collections` module: [`collections.defaultdict`](https://docs.python.org/3/library/collections.html?highlight=defaultdict#collections.defaultdict).\n",
    "\n",
    "When you create a default dictionary, you need to provide a \"factory\" function that the dictionary can use to create an initial value when the key does *not* exist. For instance, in the preceding example, when the key was not present the code creates a new key with the initial value of an integer zero (0). Indeed, this default value is the one you get when you call `int()` with no arguments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6592cfa8208f3f4c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print (int ())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6f9dc2b535bb2277",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'existing-key': 6, 'new-key': 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "D2 = defaultdict (int) # Empty dictionary\n",
    "\n",
    "D2['existing-key'] = 5 # Create one key-value pair\n",
    "\n",
    "D2['existing-key'] += 1 # Update\n",
    "D2['new-key'] += 1\n",
    "\n",
    "print (D2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7478c14e721f5546",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "**Exercise 4** (`update_pair_counts_test`: 2 points). Start by implementing a function that enumerates all item-pairs within an itemset and updates, _in-place_, a table that tracks the counts of those item-pairs.\n",
    "\n",
    "The signature of this function is:\n",
    "\n",
    "```python\n",
    "   def update_pair_counts(pair_counts, itemset):\n",
    "       ...\n",
    "```\n",
    "\n",
    "where you `pair_counts` is the table to update and `itemset` is the itemset from which you need to enumerate item-pairs. You may assume `pair_counts` is a default dictionary. Each key is a pair of items `(a, b)`, and each value is the count. You may assume all items in `itemset` are distinct, i.e., that you may treat it as you would any set-like collection. Since the function will modify `pair_counts`, it does not need to return an object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "update_pair_counts",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from itertools import permutations # Hint!\n",
    "\n",
    "def update_pair_counts (pair_counts, itemset):\n",
    "    \"\"\"\n",
    "    Updates a dictionary of pair counts for\n",
    "    all pairs of items in a given itemset.\n",
    "    \"\"\"\n",
    "    assert type (pair_counts) is defaultdict\n",
    "    for pair in list(permutations(itemset, 2)):\n",
    "        pair_counts[pair] += 1\n",
    "\n",
    "    #\n",
    "    # YOUR CODE HERE\n",
    "    #\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "update_pair_counts_test",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"{'o', 'r', 'e'}\" + \"{'o', 'r', 'l', 'd'}\"\n",
      "==> defaultdict(<class 'int'>, {('o', 'r'): 2, ('o', 'e'): 1, ('r', 'o'): 2, ('r', 'e'): 1, ('e', 'o'): 1, ('e', 'r'): 1, ('o', 'l'): 1, ('o', 'd'): 1, ('r', 'l'): 1, ('r', 'd'): 1, ('l', 'o'): 1, ('l', 'r'): 1, ('l', 'd'): 1, ('d', 'o'): 1, ('d', 'r'): 1, ('d', 'l'): 1})\n",
      "\n",
      "(Passed!)\n"
     ]
    }
   ],
   "source": [
    "# `update_pair_counts_test`: Test cell\n",
    "itemset_1 = set(\"error\")\n",
    "itemset_2 = set(\"dolor\")\n",
    "pair_counts = defaultdict(int)\n",
    "\n",
    "update_pair_counts(pair_counts, itemset_1)\n",
    "assert len(pair_counts) == 6\n",
    "update_pair_counts(pair_counts, itemset_2)\n",
    "assert len(pair_counts) == 16\n",
    "\n",
    "print('\"{}\" + \"{}\"\\n==> {}'.format (itemset_1, itemset_2, pair_counts))\n",
    "for a, b in pair_counts:\n",
    "    assert (b, a) in pair_counts\n",
    "    assert pair_counts[(a, b)] == pair_counts[(b, a)]\n",
    "    \n",
    "print (\"\\n(Passed!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-138bdd5a6eecdbf3",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "**Exercise 5** (`update_item_counts_test`: 2 points). Implement a procedure that, given an itemset, updates a table to track counts of each item.\n",
    "\n",
    "As with the previous exercise, you may assume all items in the given itemset (`itemset`) are distinct, i.e., that you may treat it as you would any set-like collection. You may also assume the table (`item_counts`) is a default dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "update_item_counts",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def update_item_counts(item_counts, itemset):\n",
    "    #\n",
    "    # YOUR CODE HERE\n",
    "    #\n",
    "    for key in itemset:\n",
    "        item_counts[key] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "update_item_counts_test",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(Passed!)\n"
     ]
    }
   ],
   "source": [
    "# `update_item_counts_test`: Test cell\n",
    "itemset_1 = set(\"error\")\n",
    "itemset_2 = set(\"dolor\")\n",
    "\n",
    "item_counts = defaultdict(int)\n",
    "update_item_counts(item_counts, itemset_1)\n",
    "assert len(item_counts) == 3\n",
    "update_item_counts(item_counts, itemset_2)\n",
    "assert len(item_counts) == 5\n",
    "\n",
    "assert item_counts['d'] == 1\n",
    "assert item_counts['e'] == 1\n",
    "assert item_counts['l'] == 1\n",
    "assert item_counts['o'] == 2\n",
    "assert item_counts['r'] == 2\n",
    "\n",
    "print(\"\\n(Passed!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-286805ebda025a3e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "**Exercise 6** (`filter_rules_by_conf_test`: 2 points). Given tables of item-pair counts and individual item counts, as well as a confidence threshold, return the rules that meet the threshold. The returned rules should be in the form of a dictionary whose key is the tuple, $(a, b)$ corresponding to the rule $a \\Rightarrow b$, and whose value is the confidence of the rule, $\\mathrm{conf}(a \\Rightarrow b)$.\n",
    "\n",
    "You may assume that if $(a, b)$ is in the table of item-pair counts, then both $a$ and $b$ are in the table of individual item counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "filter_rules_by_conf",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def filter_rules_by_conf (pair_counts, item_counts, threshold):\n",
    "    rules = {key: value/item_counts[key[0]] for key, value in pair_counts.items() if value/item_counts[key[0]] >= threshold} # (item_a, item_b) -> conf (item_a => item_b)\n",
    "    #\n",
    "    # YOUR CODE HERE\n",
    "    #\n",
    "    return rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "filter_rules_by_conf_test",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found these rules: {('man', 'woman'): 0.7142857142857143, ('red fish', 'blue fish'): 0.6363636363636364}\n",
      "\n",
      "(Passed!)\n"
     ]
    }
   ],
   "source": [
    "# `filter_rules_by_conf_test`: Test cell\n",
    "pair_counts = {('man', 'woman'): 5,\n",
    "               ('bird', 'bee'): 3,\n",
    "               ('red fish', 'blue fish'): 7}\n",
    "item_counts = {'man': 7,\n",
    "               'bird': 9,\n",
    "               'red fish': 11}\n",
    "rules = filter_rules_by_conf (pair_counts, item_counts, 0.5)\n",
    "print(\"Found these rules:\", rules)\n",
    "assert ('man', 'woman') in rules\n",
    "assert ('bird', 'bee') not in rules\n",
    "assert ('red fish', 'blue fish') in rules\n",
    "print(\"\\n(Passed!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f7a79702cbc24148",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "**Aside: pretty printing the rules.** The output of rules above is a little messy; here's a little helper function that structures that output a little, which will be useful for both debugging and reporting purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-69ef4998074c91ea",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conf(man => woman) = 0.714\n",
      "conf(red fish => blue fish) = 0.636\n"
     ]
    }
   ],
   "source": [
    "def gen_rule_str(a, b, val=None, val_fmt='{:.3f}', sep=\" = \"):\n",
    "    text = \"{} => {}\".format(a, b)\n",
    "    if val:\n",
    "        text = \"conf(\" + text + \")\"\n",
    "        text += sep + val_fmt.format(val)\n",
    "    return text\n",
    "\n",
    "def print_rules(rules):\n",
    "    if type(rules) is dict or type(rules) is defaultdict:\n",
    "        from operator import itemgetter\n",
    "        ordered_rules = sorted(rules.items(), key=itemgetter(1), reverse=True)\n",
    "    else: # Assume rules is iterable\n",
    "        ordered_rules = [((a, b), None) for a, b in rules]\n",
    "    for (a, b), conf_ab in ordered_rules:\n",
    "        print(gen_rule_str(a, b, conf_ab))\n",
    "\n",
    "# Demo:\n",
    "print_rules(rules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b564ebbe2fe2a051",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "**Exercise 7** (`find_assoc_rules_test`: 3 points). Using the building blocks you implemented above, complete a function `find_assoc_rules` so that it implements the basic association rule mining algorithm and returns a dictionary of rules.\n",
    "\n",
    "In particular, your implementation may assume the following:\n",
    "\n",
    "1. As indicated in its signature, below, the function takes two inputs: `receipts` and `threshold`.\n",
    "1. The input, `receipts`, is a collection of itemsets: for every receipt `r` in `receipts`, `r` may be treated as a collection of unique items.\n",
    "2. The input `threshold` is the minimum desired confidence value. That is, the function should only return rules whose confidence is at least `threshold`.\n",
    "\n",
    "The returned dictionary, `rules`, should be keyed by tuples $(a, b)$ corresponding to the rule $a \\Rightarrow b$; each value should the the confidence $\\mathrm{conf}(a \\Rightarrow b)$ of the rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "find_assoc_rules",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def find_assoc_rules(receipts, threshold, item_counts = None):\n",
    "    #\n",
    "    # YOUR CODE HERE\n",
    "    #\n",
    "    pair_counts = defaultdict(int)\n",
    "    if item_counts == None:\n",
    "        item_counts = defaultdict(int)\n",
    "    for receipt in receipts:\n",
    "        update_pair_counts(pair_counts, receipt)\n",
    "        update_item_counts(item_counts, receipt)\n",
    "    return filter_rules_by_conf(pair_counts, item_counts, threshold)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "find_assoc_rules_test",
     "locked": true,
     "points": 3,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original receipts as itemsets: [{'b', 'a', 'c'}, {'a', 'c'}, {'a'}]\n",
      "Resulting rules:\n",
      "conf(b => a) = 1.000\n",
      "conf(b => c) = 1.000\n",
      "conf(c => a) = 1.000\n",
      "conf(a => c) = 0.667\n",
      "\n",
      "(Passed!)\n"
     ]
    }
   ],
   "source": [
    "# `find_assoc_rules_test`: Test cell\n",
    "receipts = [set('abbc'), set('ac'), set('a')]\n",
    "rules = find_assoc_rules(receipts, 0.6)\n",
    "\n",
    "print(\"Original receipts as itemsets:\", receipts)\n",
    "print(\"Resulting rules:\")\n",
    "print_rules(rules)\n",
    "\n",
    "assert ('a', 'b') not in rules\n",
    "assert ('b', 'a') in rules\n",
    "assert ('a', 'c') in rules\n",
    "assert ('c', 'a') in rules\n",
    "assert ('b', 'c') in rules\n",
    "assert ('c', 'b') not in rules\n",
    "\n",
    "print(\"\\n(Passed!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-706f5b5bb1dbe542",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "**Exercise 8** (`latin_rules_test`: 2 points). For the Latin string, `latin_text`, use your `find_assoc_rules()` function to compute the rules whose confidence is at least 0.75. Store your result in a variable named `latin_rules`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "latin_rules",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conf(q => u) = 1.000\n",
      "conf(x => e) = 1.000\n",
      "conf(h => i) = 0.833\n",
      "conf(x => i) = 0.833\n",
      "conf(v => t) = 0.818\n",
      "conf(r => e) = 0.800\n",
      "conf(v => e) = 0.773\n",
      "conf(b => i) = 0.750\n",
      "conf(g => i) = 0.750\n",
      "conf(f => i) = 0.750\n"
     ]
    }
   ],
   "source": [
    "# Generate `latin_rules`:\n",
    "#\n",
    "# YOUR CODE HERE\n",
    "#\n",
    "latin_rules = find_assoc_rules(norm_latin_itemsets, 0.75)\n",
    "# Inspect your result:\n",
    "print_rules(latin_rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "latin_rules_test",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(Passed!)\n"
     ]
    }
   ],
   "source": [
    "# `latin_rules_test`: Test cell\n",
    "assert len(latin_rules) == 10\n",
    "assert all([0.75 <= v <= 1.0 for v in latin_rules.values()])\n",
    "for ab in ['xe', 'qu', 'hi', 'xi', 'vt', 're', 've', 'fi', 'gi', 'bi']:\n",
    "    assert (ab[0], ab[1]) in latin_rules\n",
    "print(\"\\n(Passed!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-73a1d629f6548461",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Next, let's analyze the rules common to Latin text _and_ English text. That is, suppose we have two lists of commonly occurring rules, one for Latin text (computed above as `latin_rules`) and one for English text; we'd like to know which pairs commonly occur in both.\n",
    "\n",
    "For the English text, here is an English translation of the _lorem ipsum_ text, encoded as the variable `english_text` in the next code cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7f545767297a391a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "english_text = \"\"\"\n",
    "But I must explain to you how all this mistaken idea\n",
    "of denouncing of a pleasure and praising pain was\n",
    "born and I will give you a complete account of the\n",
    "system, and expound the actual teachings of the great\n",
    "explorer of the truth, the master-builder of human\n",
    "happiness. No one rejects, dislikes, or avoids\n",
    "pleasure itself, because it is pleasure, but because\n",
    "those who do not know how to pursue pleasure\n",
    "rationally encounter consequences that are extremely\n",
    "painful. Nor again is there anyone who loves or\n",
    "pursues or desires to obtain pain of itself, because\n",
    "it is pain, but occasionally circumstances occur in\n",
    "which toil and pain can procure him some great\n",
    "pleasure. To take a trivial example, which of us\n",
    "ever undertakes laborious physical exercise, except\n",
    "to obtain some advantage from it? But who has any\n",
    "right to find fault with a man who chooses to enjoy\n",
    "a pleasure that has no annoying consequences, or\n",
    "one who avoids a pain that produces no resultant\n",
    "pleasure?\n",
    "\n",
    "On the other hand, we denounce with righteous\n",
    "indignation and dislike men who are so beguiled and\n",
    "demoralized by the charms of pleasure of the moment,\n",
    "so blinded by desire, that they cannot foresee the\n",
    "pain and trouble that are bound to ensue; and equal\n",
    "blame belongs to those who fail in their duty\n",
    "through weakness of will, which is the same as\n",
    "saying through shrinking from toil and pain. These\n",
    "cases are perfectly simple and easy to distinguish.\n",
    "In a free hour, when our power of choice is\n",
    "untrammeled and when nothing prevents our being\n",
    "able to do what we like best, every pleasure is to\n",
    "be welcomed and every pain avoided. But in certain\n",
    "circumstances and owing to the claims of duty or\n",
    "the obligations of business it will frequently\n",
    "occur that pleasures have to be repudiated and\n",
    "annoyances accepted. The wise man therefore always\n",
    "holds in these matters to this principle of\n",
    "selection: he rejects pleasures to secure other\n",
    "greater pleasures, or else he endures pains to\n",
    "avoid worse pains.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-aaff63d41d0ad432",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "**Exercise 9** (`intersect_keys_test`: 2 points). Write a function that, given two dictionaries, finds the intersection of their keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "intersect_keys",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def intersect_keys(d1, d2):\n",
    "    assert type(d1) is dict or type(d1) is defaultdict\n",
    "    assert type(d2) is dict or type(d2) is defaultdict\n",
    "    #\n",
    "    # YOUR CODE HERE\n",
    "    #\n",
    "    return list(set(d1.keys()) & set(d2.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "intersect_keys_test",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(Passed!)\n"
     ]
    }
   ],
   "source": [
    "# `intersect_keys_test`: Test cell\n",
    "from random import sample\n",
    "\n",
    "key_space = {'ape', 'baboon', 'bonobo', 'chimp', 'gorilla', 'monkey', 'orangutan'}\n",
    "val_space = range(100)\n",
    "\n",
    "for trial in range(10): # Try 10 random tests\n",
    "    d1 = {k: v for k, v in zip(sample(key_space, 4), sample(val_space, 4))}\n",
    "    d2 = {k: v for k, v in zip(sample(key_space, 3), sample(val_space, 3))}\n",
    "    k_common = intersect_keys(d1, d2)\n",
    "    for k in key_space:\n",
    "        is_common = (k in k_common) and (k in d1) and (k in d2)\n",
    "        is_not_common = (k not in k_common) and ((k not in d1) or (k not in d2))\n",
    "        assert is_common or is_not_common\n",
    "        \n",
    "print(\"\\n(Passed!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-24b55c6bcbe30dc0",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "**Exercise 10** (`common_high_conf_rules_test`: 1 points). Let's consider any rules with a confidence of at least 0.75 to be a \"high-confidence rule.\"\n",
    "\n",
    "Write some code that finds all high-confidence rules appearing in _both_ the Latin text _and_ the English text. Store your result in a list named `common_high_conf_rules` whose elements are $(a, b)$ pairs corresponding to the rules $a \\Rightarrow b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# YOUR CODE HERE\n",
    "#\n",
    "norm_english_text = normalize_string(english_text)\n",
    "norm_english_words = get_normalized_words(norm_english_text)\n",
    "norm_english_itemsets = make_itemsets(norm_english_words)\n",
    "english_rules = find_assoc_rules(norm_english_itemsets, 0.75)\n",
    "common_high_conf_rules = intersect_keys(latin_rules, english_rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "common_high_conf_rules",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[113] voluptate --> {'e', 't', 'a', 'v', 'o', 'p', 'l', 'u'}\n",
      "[200] facere --> {'e', 'a', 'c', 'f', 'r'}\n",
      "[10] accusantium --> {'t', 'i', 's', 'n', 'm', 'a', 'c', 'u'}\n",
      "[107] eum --> {'u', 'm', 'e'}\n",
      "[205] est --> {'t', 's', 'e'}\n",
      "\n",
      "(Passed!)\n",
      "\n",
      "(Passed.)\n",
      "\n",
      "(Passed!)\n",
      "High-confidence rules common to _lorem ipsum_ in Latin and English:\n",
      "x => e\n",
      "q => u\n"
     ]
    }
   ],
   "source": [
    "# Lists should have the same size\n",
    "assert len(norm_latin_itemsets) == len(norm_latin_words)\n",
    "\n",
    "# Test a random sample\n",
    "from random import sample\n",
    "for i in sample(range(len(norm_latin_words)), 5):\n",
    "    print('[{}]'.format(i), norm_latin_words[i], \"-->\", norm_latin_itemsets[i])\n",
    "    assert set(norm_latin_words[i]) == norm_latin_itemsets[i]\n",
    "print(\"\\n(Passed!)\")\n",
    "\n",
    "assert len(norm_latin_words) == 250\n",
    "for i, w in [(20, 'illo'), (73, 'eius'), (144, 'deleniti'), (248, 'asperiores')]:\n",
    "    assert norm_latin_words[i] == w\n",
    "\n",
    "print (\"\\n(Passed.)\")\n",
    "assert type(norm_latin_text) is str\n",
    "assert len(norm_latin_text) == 1694\n",
    "assert all([c.isalpha() or c.isspace() for c in norm_latin_text])\n",
    "assert norm_latin_text == norm_latin_text.lower()\n",
    "\n",
    "print(\"\\n(Passed!)\")\n",
    "\n",
    "print(\"High-confidence rules common to _lorem ipsum_ in Latin and English:\")\n",
    "print_rules(common_high_conf_rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "common_high_conf_rules_test",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(Passed!)\n"
     ]
    }
   ],
   "source": [
    "# `common_high_conf_rules_test`: Test cell\n",
    "assert len(common_high_conf_rules) == 2\n",
    "assert ('x', 'e') in common_high_conf_rules\n",
    "assert ('q', 'u') in common_high_conf_rules\n",
    "print(\"\\n(Passed!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8b001f7cf9d3d952",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Putting it all together: Actual baskets!\n",
    "\n",
    "Let's take a look at some real data that [someone](http://www.salemmarafi.com/code/market-basket-analysis-with-r/) was kind enough to prepare for a similar exercise designed for the R programming environment.\n",
    "\n",
    "First, here's a code snippet to load the data, which is a text file. If you are running in the Vocareum environment, we've already placed a copy of the data there; if you are running outside, this code will try to download a copy from the CSE 6040 website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-682648842e4c0f47",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'groceries.csv' is ready!\n",
      "citrus fruit,semi-finished bread,margarine,ready soups\n",
      "tropical fruit,yogurt,coffee\n",
      "whole milk\n",
      "pip fruit,yogurt,cream cheese ,meat spreads\n",
      "other vegetables,whole milk,condensed milk,long life bakery product\n",
      "whole milk,butter,yogurt,rice,abrasive clea...\n",
      "... (etc.) ...\n",
      "\n",
      "(All data appears to be ready.)\n"
     ]
    }
   ],
   "source": [
    "def on_vocareum():\n",
    "    import os\n",
    "    return os.path.exists('.voc')\n",
    "\n",
    "def download(file, local_dir=\"\", url_base=None, checksum=None):\n",
    "    import os, requests, hashlib, io\n",
    "    local_file = \"{}{}\".format(local_dir, file)\n",
    "    if not os.path.exists(local_file):\n",
    "        if url_base is None:\n",
    "            url_base = \"https://cse6040.gatech.edu/datasets/\"\n",
    "        url = \"{}{}\".format(url_base, file)\n",
    "        print(\"Downloading: {} ...\".format(url))\n",
    "        r = requests.get(url)\n",
    "        with open(local_file, 'wb') as f:\n",
    "            f.write(r.content)            \n",
    "    if checksum is not None:\n",
    "        with io.open(local_file, 'rb') as f:\n",
    "            body = f.read()\n",
    "            body_checksum = hashlib.md5(body).hexdigest()\n",
    "            assert body_checksum == checksum, \\\n",
    "                \"Downloaded file '{}' has incorrect checksum: '{}' instead of '{}'\".format(local_file,\n",
    "                                                                                           body_checksum,\n",
    "                                                                                           checksum)\n",
    "    print(\"'{}' is ready!\".format(file))\n",
    "    \n",
    "if on_vocareum():\n",
    "    DATA_PATH = \"../resource/asnlib/publicdata/\"\n",
    "else:\n",
    "    DATA_PATH = \"\"\n",
    "datasets = {'groceries.csv': '0a3d21c692be5c8ce55c93e59543dcbe'}\n",
    "\n",
    "for filename, checksum in datasets.items():\n",
    "    download(filename, local_dir=DATA_PATH, checksum=checksum)\n",
    "\n",
    "with open('{}{}'.format(DATA_PATH, 'groceries.csv')) as fp:\n",
    "    groceries_file = fp.read()\n",
    "print (groceries_file[0:250] + \"...\\n... (etc.) ...\") # Prints the first 250 characters only\n",
    "print(\"\\n(All data appears to be ready.)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5f24415cdf3c20e8",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Each line of this file is some customer's shopping basket. The items that the customer bought are stored as a comma-separated list of values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-cd51e1426544e8bd",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "**Exercise 11: Your task.** (`basket_rules_test`: 4 points). Your final task in this notebook is to mine this dataset for pairwise association rules. In particular, your code should produce (no pun intended!) a final dictionary, `basket_rules`, that meet these conditions (read carefully!):\n",
    "\n",
    "1. The keys are pairs $(a, b)$, where $a$ and $b$ are item names (as strings).\n",
    "2. The values are the corresponding confidence scores, $\\mathrm{conf}(a \\Rightarrow b)$.\n",
    "3. Only include rules $a \\Rightarrow b$ where item $a$ occurs at least `MIN_COUNT` times and $\\mathrm{conf}(a \\Rightarrow b)$ is at least `THRESHOLD`.\n",
    "\n",
    "Pay particular attention to Condition 3: not only do you have to filter by a confidence threshold, but you must exclude rules $a \\Rightarrow b$ where the item $a$ does not appear \"often enough.\" There is a code cell below that defines values of `MIN_COUNT` and `THRESHOLD`, but your code should work even if we decide to change those values later on.\n",
    "\n",
    "> _Aside_: Why would an analyst want to enforce Condition 3?\n",
    "\n",
    "Your solution can use the `groceries_file` string variable defined above as its starting point. And since it's in the same notebook, you may, of course, reuse any of the code you've written above as needed. Lastly, if you feel you need additional code cells, you can create them _after_ the code cell marked for your solution but _before_ the code marked, `### TEST CODE ###`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-debb3de1a73e4495",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Confidence threshold\n",
    "THRESHOLD = 0.5\n",
    "\n",
    "# Only consider rules for items appearing at least `MIN_COUNT` times.\n",
    "MIN_COUNT = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filter_rules_by_conf_new(pair_counts, item_counts, threshold):\n",
    "    rules = {}\n",
    "    for key, value in pair_counts.items():\n",
    "        if key[0] in item_counts:\n",
    "            if value/item_counts[key[0]] >= threshold:\n",
    "                rules[key] = value/item_counts[key[0]]\n",
    "    return rules\n",
    "\n",
    "def find_assoc_rules_new(receipts, threshold, MIN_COUNT):\n",
    "    #\n",
    "    # YOUR CODE HERE\n",
    "    #\n",
    "    pair_counts = defaultdict(int)\n",
    "    item_counts = defaultdict(int)\n",
    "    for receipt in receipts:\n",
    "        update_pair_counts(pair_counts, receipt)\n",
    "        update_item_counts(item_counts, receipt)\n",
    "    item_counts = {k:v for k,v in item_counts.items() if v>= MIN_COUNT}\n",
    "    return filter_rules_by_conf_new(pair_counts, item_counts, threshold)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "basket_rules",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# YOUR CODE HERE\n",
    "#\n",
    "baskets = [basket.split(\",\") for basket in groceries_file.splitlines()]\n",
    "basket_rules = find_assoc_rules_new(baskets, THRESHOLD, MIN_COUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "basket_rules_test",
     "locked": true,
     "points": 4,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 19 rules whose confidence exceeds 0.5.\n",
      "Here they are:\n",
      "\n",
      "conf(honey => whole milk) = 0.733\n",
      "conf(frozen fruits => other vegetables) = 0.667\n",
      "conf(cereals => whole milk) = 0.643\n",
      "conf(rice => whole milk) = 0.613\n",
      "conf(rubbing alcohol => whole milk) = 0.600\n",
      "conf(cocoa drinks => whole milk) = 0.591\n",
      "conf(pudding powder => whole milk) = 0.565\n",
      "conf(jam => whole milk) = 0.547\n",
      "conf(cream => other vegetables) = 0.538\n",
      "conf(cream => sausage) = 0.538\n",
      "conf(baking powder => whole milk) = 0.523\n",
      "conf(tidbits => rolls/buns) = 0.522\n",
      "conf(rice => other vegetables) = 0.520\n",
      "conf(cooking chocolate => whole milk) = 0.520\n",
      "conf(specialty cheese => other vegetables) = 0.500\n",
      "conf(rubbing alcohol => citrus fruit) = 0.500\n",
      "conf(rubbing alcohol => butter) = 0.500\n",
      "conf(ready soups => rolls/buns) = 0.500\n",
      "conf(frozen fruits => whipped/sour cream) = 0.500\n",
      "\n",
      "(Passed!)\n"
     ]
    }
   ],
   "source": [
    "### `basket_rules_test`: TEST CODE ###\n",
    "print(\"Found {} rules whose confidence exceeds {}.\".format(len(basket_rules), THRESHOLD))\n",
    "print(\"Here they are:\\n\")\n",
    "print_rules(basket_rules)\n",
    "\n",
    "assert len(basket_rules) == 19\n",
    "assert all([THRESHOLD <= v < 1.0 for v in basket_rules.values()])\n",
    "ans_keys = [(\"pudding powder\", \"whole milk\"), (\"tidbits\", \"rolls/buns\"), (\"cocoa drinks\", \"whole milk\"), (\"cream\", \"sausage\"), (\"rubbing alcohol\", \"whole milk\"), (\"honey\", \"whole milk\"), (\"frozen fruits\", \"other vegetables\"), (\"cream\", \"other vegetables\"), (\"ready soups\", \"rolls/buns\"), (\"cooking chocolate\", \"whole milk\"), (\"cereals\", \"whole milk\"), (\"rice\", \"whole milk\"), (\"specialty cheese\", \"other vegetables\"), (\"baking powder\", \"whole milk\"), (\"rubbing alcohol\", \"butter\"), (\"rubbing alcohol\", \"citrus fruit\"), (\"jam\", \"whole milk\"), (\"frozen fruits\", \"whipped/sour cream\"), (\"rice\", \"other vegetables\")]\n",
    "for k in ans_keys:\n",
    "    assert k in basket_rules\n",
    "\n",
    "print(\"\\n(Passed!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "**Fin!** Don't forget to restart the kernel and re-run the notebook from scratch. If that seems to work, go ahead and submit the notebook in the autograder."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": [],
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
